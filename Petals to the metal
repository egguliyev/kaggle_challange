{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":21154,"databundleVersionId":1243559,"sourceType":"competition"}],"dockerImageVersionId":30734,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, Input, Conv2D, MaxPooling2D\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom sklearn.metrics import f1_score\nfrom tensorflow.keras.models import Model\nimport os\n\n# Detect TPU\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\n# Connect to TPU\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)  # Updated to non-experimental TPUStrategy\nelse:\n    strategy = tf.distribute.get_strategy()  # Default strategy for CPU and single GPU","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-21T05:12:32.136877Z","iopub.execute_input":"2024-09-21T05:12:32.137498Z","iopub.status.idle":"2024-09-21T05:12:55.165164Z","shell.execute_reply.started":"2024-09-21T05:12:32.137466Z","shell.execute_reply":"2024-09-21T05:12:55.164412Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1726895559.460107      13 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:479\nD0921 05:12:39.468235390      13 config.cc:196]                        gRPC EXPERIMENT call_status_override_on_cancellation   OFF (default:OFF)\nD0921 05:12:39.468249158      13 config.cc:196]                        gRPC EXPERIMENT call_v3                                OFF (default:OFF)\nD0921 05:12:39.468252595      13 config.cc:196]                        gRPC EXPERIMENT canary_client_privacy                  ON  (default:ON)\nD0921 05:12:39.468255177      13 config.cc:196]                        gRPC EXPERIMENT capture_base_context                   ON  (default:ON)\nD0921 05:12:39.468257582      13 config.cc:196]                        gRPC EXPERIMENT client_idleness                        ON  (default:ON)\nD0921 05:12:39.468259857      13 config.cc:196]                        gRPC EXPERIMENT client_privacy                         ON  (default:ON)\nD0921 05:12:39.468262157      13 config.cc:196]                        gRPC EXPERIMENT dapper_request_wire_size               OFF (default:OFF)\nD0921 05:12:39.468264358      13 config.cc:196]                        gRPC EXPERIMENT empty_experiment                       OFF (default:OFF)\nD0921 05:12:39.468266492      13 config.cc:196]                        gRPC EXPERIMENT event_engine_client                    OFF (default:OFF)\nD0921 05:12:39.468268611      13 config.cc:196]                        gRPC EXPERIMENT event_engine_dns                       ON  (default:ON)\nD0921 05:12:39.468270773      13 config.cc:196]                        gRPC EXPERIMENT event_engine_listener                  ON  (default:ON)\nD0921 05:12:39.468272919      13 config.cc:196]                        gRPC EXPERIMENT free_large_allocator                   OFF (default:OFF)\nD0921 05:12:39.468275055      13 config.cc:196]                        gRPC EXPERIMENT google_no_envelope_resolver            OFF (default:OFF)\nD0921 05:12:39.468277185      13 config.cc:196]                        gRPC EXPERIMENT http2_stats_fix                        OFF (default:OFF)\nD0921 05:12:39.468279317      13 config.cc:196]                        gRPC EXPERIMENT keepalive_fix                          OFF (default:OFF)\nD0921 05:12:39.468281460      13 config.cc:196]                        gRPC EXPERIMENT keepalive_server_fix                   ON  (default:ON)\nD0921 05:12:39.468283742      13 config.cc:196]                        gRPC EXPERIMENT loas_do_not_prefer_rekey_next_protocol OFF (default:OFF)\nD0921 05:12:39.468285877      13 config.cc:196]                        gRPC EXPERIMENT loas_prod_to_cloud_prefer_pfs_ciphers  OFF (default:OFF)\nD0921 05:12:39.468288055      13 config.cc:196]                        gRPC EXPERIMENT monitoring_experiment                  ON  (default:ON)\nD0921 05:12:39.468290241      13 config.cc:196]                        gRPC EXPERIMENT multiping                              OFF (default:OFF)\nD0921 05:12:39.468292393      13 config.cc:196]                        gRPC EXPERIMENT peer_state_based_framing               OFF (default:OFF)\nD0921 05:12:39.468294567      13 config.cc:196]                        gRPC EXPERIMENT pending_queue_cap                      ON  (default:ON)\nD0921 05:12:39.468296810      13 config.cc:196]                        gRPC EXPERIMENT pick_first_happy_eyeballs              ON  (default:ON)\nD0921 05:12:39.468298981      13 config.cc:196]                        gRPC EXPERIMENT promise_based_client_call              OFF (default:OFF)\nD0921 05:12:39.468301077      13 config.cc:196]                        gRPC EXPERIMENT promise_based_inproc_transport         OFF (default:OFF)\nD0921 05:12:39.468303183      13 config.cc:196]                        gRPC EXPERIMENT promise_based_server_call              OFF (default:OFF)\nD0921 05:12:39.468305406      13 config.cc:196]                        gRPC EXPERIMENT registered_method_lookup_in_transport  ON  (default:ON)\nD0921 05:12:39.468307558      13 config.cc:196]                        gRPC EXPERIMENT rfc_max_concurrent_streams             ON  (default:ON)\nD0921 05:12:39.468309815      13 config.cc:196]                        gRPC EXPERIMENT round_robin_delegate_to_pick_first     ON  (default:ON)\nD0921 05:12:39.468312907      13 config.cc:196]                        gRPC EXPERIMENT rstpit                                 OFF (default:OFF)\nD0921 05:12:39.468315167      13 config.cc:196]                        gRPC EXPERIMENT schedule_cancellation_over_write       OFF (default:OFF)\nD0921 05:12:39.468317484      13 config.cc:196]                        gRPC EXPERIMENT server_privacy                         ON  (default:ON)\nD0921 05:12:39.468319831      13 config.cc:196]                        gRPC EXPERIMENT tcp_frame_size_tuning                  OFF (default:OFF)\nD0921 05:12:39.468321992      13 config.cc:196]                        gRPC EXPERIMENT tcp_rcv_lowat                          OFF (default:OFF)\nD0921 05:12:39.468324132      13 config.cc:196]                        gRPC EXPERIMENT trace_record_callops                   OFF (default:OFF)\nD0921 05:12:39.468326280      13 config.cc:196]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size    OFF (default:OFF)\nD0921 05:12:39.468328408      13 config.cc:196]                        gRPC EXPERIMENT v3_backend_metric_filter               OFF (default:OFF)\nD0921 05:12:39.468330560      13 config.cc:196]                        gRPC EXPERIMENT v3_channel_idle_filters                ON  (default:ON)\nD0921 05:12:39.468332780      13 config.cc:196]                        gRPC EXPERIMENT v3_compression_filter                  ON  (default:ON)\nD0921 05:12:39.468334930      13 config.cc:196]                        gRPC EXPERIMENT v3_server_auth_filter                  OFF (default:OFF)\nD0921 05:12:39.468337062      13 config.cc:196]                        gRPC EXPERIMENT work_serializer_clears_time_cache      OFF (default:OFF)\nD0921 05:12:39.468339165      13 config.cc:196]                        gRPC EXPERIMENT work_serializer_dispatch               OFF (default:OFF)\nD0921 05:12:39.468341350      13 config.cc:196]                        gRPC EXPERIMENT write_size_cap                         ON  (default:ON)\nD0921 05:12:39.468343546      13 config.cc:196]                        gRPC EXPERIMENT write_size_policy                      ON  (default:ON)\nD0921 05:12:39.468345737      13 config.cc:196]                        gRPC EXPERIMENT wrr_delegate_to_pick_first             ON  (default:ON)\nI0921 05:12:39.468498020      13 ev_epoll1_linux.cc:123]               grpc epoll fd: 59\nD0921 05:12:39.468533142      13 ev_posix.cc:113]                      Using polling engine: epoll1\nD0921 05:12:39.478664897      13 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0921 05:12:39.478675257      13 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0921 05:12:39.478682520      13 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0921 05:12:39.478685620      13 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0921 05:12:39.478688537      13 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0921 05:12:39.478691215      13 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin\"\nD0921 05:12:39.478717585      13 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0921 05:12:39.478729016      13 dns_resolver_plugin.cc:43]            Using EventEngine dns resolver\nD0921 05:12:39.478747907      13 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0921 05:12:39.478767712      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0921 05:12:39.478774464      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0921 05:12:39.478777408      13 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0921 05:12:39.478781090      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0921 05:12:39.478784062      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0921 05:12:39.478787290      13 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0921 05:12:39.478790541      13 certificate_provider_registry.cc:33]  registering certificate provider factory for \"file_watcher\"\nD0921 05:12:39.478821704      13 channel_init.cc:157]                  Filter server-auth not registered, but is referenced in the after clause of grpc-server-authz when building channel stack SERVER_CHANNEL\nI0921 05:12:39.480735121      13 ev_epoll1_linux.cc:359]               grpc epoll fd: 61\nI0921 05:12:39.481881262      13 tcp_socket_utils.cc:689]              Disabling AF_INET6 sockets because ::1 is not available.\nI0921 05:12:39.497411717     102 socket_utils_common_posix.cc:452]     Disabling AF_INET6 sockets because ::1 is not available.\nI0921 05:12:39.497456559     102 socket_utils_common_posix.cc:379]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE0921 05:12:39.503152176      13 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-09-21T05:12:39.503137487+00:00\", grpc_status:2}\n","output_type":"stream"},{"name":"stdout","text":"Running on TPU  \nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1726895571.214935      13 service.cc:145] XLA service 0x5a05beaaaeb0 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1726895571.214984      13 service.cc:153]   StreamExecutor device (0): TPU, 2a886c8\nI0000 00:00:1726895571.214988      13 service.cc:153]   StreamExecutor device (1): TPU, 2a886c8\nI0000 00:00:1726895571.214991      13 service.cc:153]   StreamExecutor device (2): TPU, 2a886c8\nI0000 00:00:1726895571.214994      13 service.cc:153]   StreamExecutor device (3): TPU, 2a886c8\nI0000 00:00:1726895571.214997      13 service.cc:153]   StreamExecutor device (4): TPU, 2a886c8\nI0000 00:00:1726895571.214999      13 service.cc:153]   StreamExecutor device (5): TPU, 2a886c8\nI0000 00:00:1726895571.215002      13 service.cc:153]   StreamExecutor device (6): TPU, 2a886c8\nI0000 00:00:1726895571.215004      13 service.cc:153]   StreamExecutor device (7): TPU, 2a886c8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\nimport datetime\n\ncheckpoint_path = \"/kaggle/working/model_checkpoint.keras\"\ncheckpoint_callback = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, verbose=1)\nearly_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.0001)\nlog_dir = \"/kaggle/working/logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n\ncallbacks = [checkpoint_callback, early_stopping_callback, tensorboard_callback, reduce_lr]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T05:12:55.166360Z","iopub.execute_input":"2024-09-21T05:12:55.166599Z","iopub.status.idle":"2024-09-21T05:12:55.173751Z","shell.execute_reply.started":"2024-09-21T05:12:55.166573Z","shell.execute_reply":"2024-09-21T05:12:55.172944Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Configuration\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nIMAGE_SIZE = [224, 224]  # Specify the image size\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\n\n# Data access\nGCS_PATH = '/kaggle/input/flower-classification-with-tpus'\ntrain_dir = '/kaggle/input/tpu-getting-started/tfrecords-jpeg-224x224/train'\nval_dir = '/kaggle/input/tpu-getting-started/tfrecords-jpeg-224x224/val'\ntest_dir = '/kaggle/input/tpu-getting-started/tfrecords-jpeg-224x224/test'\n\nprint(\"Training files:\", os.listdir(train_dir)[:5])  # List first 5 training files\nprint(\"Validation files:\", os.listdir(val_dir)[:5])  # List first 5 validation files\nprint(\"Test files:\", os.listdir(test_dir)[:5])  # List first 5 test files\n\n# Load the data\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.cast(image, tf.float32) / 255.0\n    return image\n\ndef read_tfrecord(example, labeled=True):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"class\": tf.io.FixedLenFeature([], tf.int64)\n    }\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"id\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT if labeled else UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    if labeled:\n        label = tf.cast(example['class'], tf.int32)\n        return image, label\n    idnum = example['id']\n    return image, idnum\n\ndef load_dataset(filenames, labeled=True):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(lambda x: read_tfrecord(x, labeled), num_parallel_calls=AUTOTUNE)\n    return dataset\n\n# Define file paths\nTRAINING_FILENAMES = tf.io.gfile.glob(train_dir + '/*.tfrec')\nVALIDATION_FILENAMES = tf.io.gfile.glob(val_dir + '/*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(test_dir + '/*.tfrec')\n\n# Verify data loading\ntrain_dataset = load_dataset(TRAINING_FILENAMES, labeled=True).take(1)\nvalid_dataset = load_dataset(VALIDATION_FILENAMES, labeled=True).take(1)\ntest_dataset = load_dataset(TEST_FILENAMES, labeled=False).take(1)\n\nfor image, label in train_dataset:\n    print(\"Train image shape:\", image.numpy().shape)\n    print(\"Train label:\", label.numpy())\n\nfor image, label in valid_dataset:\n    print(\"Validation image shape:\", image.numpy().shape)\n    print(\"Validation label:\", label.numpy())\n\nfor image, idnum in test_dataset:\n    print(\"Test image shape:\", image.numpy().shape)\n    print(\"Test id:\", idnum.numpy().decode('utf-8'))\n\n# Data pipeline\ndef get_dataset(filenames, shuffle=False, repeat=False, labeled=True, batch_size=BATCH_SIZE):\n    dataset = load_dataset(filenames, labeled)\n    if shuffle:\n        dataset = dataset.shuffle(2048)\n    if repeat:\n        dataset = dataset.repeat()\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n    return dataset\n\n# Load datasets\ntrain_dataset = get_dataset(TRAINING_FILENAMES, shuffle=True, repeat=True)\nvalid_dataset = get_dataset(VALIDATION_FILENAMES, repeat=True)\ntest_dataset = get_dataset(TEST_FILENAMES, repeat=False, labeled=False)\n\n# Calculate number of steps per epoch\ndef count_data_items(filenames):\n    return np.sum([tf.data.TFRecordDataset(f).reduce(0, lambda x, _: x + 1).numpy() for f in filenames])\n\nnum_train_samples = count_data_items(TRAINING_FILENAMES)\nnum_val_samples = count_data_items(VALIDATION_FILENAMES)\nsteps_per_epoch = num_train_samples // BATCH_SIZE\nvalidation_steps = num_val_samples // BATCH_SIZE\n\nprint(f\"Number of training samples: {num_train_samples}\")\nprint(f\"Number of validation samples: {num_val_samples}\")\nprint(f\"Steps per epoch: {steps_per_epoch}\")\nprint(f\"Validation steps: {validation_steps}\")\n\n# Build the model\nwith strategy.scope():\n    model1 = Sequential([\n        EfficientNetB0(input_shape=(*IMAGE_SIZE, 3), include_top=False, weights='imagenet'),\n        Flatten(),\n        Dense(256, activation='relu'),\n        Dropout(0.3),\n        Dense(104, activation='softmax')\n    ])\n    model1.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy']\n    )\n\n    # Train the model\n    EPOCHS = 50\n    history = model1.fit(\n        train_dataset,\n        validation_data=valid_dataset,\n        epochs=EPOCHS,\n        steps_per_epoch=steps_per_epoch,\n        validation_steps=validation_steps, shuffle=True, callbacks=callbacks\n    )","metadata":{"execution":{"iopub.status.busy":"2024-09-21T05:12:55.174890Z","iopub.execute_input":"2024-09-21T05:12:55.175229Z","iopub.status.idle":"2024-09-21T05:16:45.799867Z","shell.execute_reply.started":"2024-09-21T05:12:55.175200Z","shell.execute_reply":"2024-09-21T05:16:45.798798Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Training files: ['13-224x224-798.tfrec', '11-224x224-798.tfrec', '05-224x224-798.tfrec', '00-224x224-798.tfrec', '02-224x224-798.tfrec']\nValidation files: ['09-224x224-232.tfrec', '14-224x224-232.tfrec', '01-224x224-232.tfrec', '13-224x224-232.tfrec', '00-224x224-232.tfrec']\nTest files: ['04-224x224-462.tfrec', '06-224x224-462.tfrec', '05-224x224-462.tfrec', '08-224x224-462.tfrec', '15-224x224-452.tfrec']\nTrain image shape: (224, 224, 3)\nTrain label: 70\nValidation image shape: (224, 224, 3)\nValidation label: 62\nTest image shape: (224, 224, 3)\nTest id: 59d1b6146\nNumber of training samples: 12753\nNumber of validation samples: 3712\nSteps per epoch: 99\nValidation steps: 29\nDownloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1726895591.329683      13 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"2024-09-21 05:14:32.449839: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node StatefulPartitionedCall.\nI0000 00:00:1726895675.288277     837 tpu_compilation_cache_interface.cc:441] TPU host compilation cache miss: cache_key(8a51f60175a32dd4:0:0), session_name()\nI0000 00:00:1726895697.202843     837 tpu_compile_op_common.cc:245] Compilation of 8a51f60175a32dd4:0:0 with session name  took 21.914510389s and succeeded\nI0000 00:00:1726895697.297685     837 tpu_compilation_cache_interface.cc:475] TPU host compilation cache: compilation complete for cache_key(8a51f60175a32dd4:0:0), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_iterator_13103382528721622100\", property.function_library_fingerprint = 11367202521057780088, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"16,224,224,3,;16,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1726895697.297745     837 tpu_compilation_cache_interface.cc:541] After adding entry for key 8a51f60175a32dd4:0:0 with session_name  cache is 1 entries (125758648 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 5.1206 - sparse_categorical_accuracy: 0.1459","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1726895713.568638     825 tpu_compilation_cache_interface.cc:441] TPU host compilation cache miss: cache_key(8a1379109dc9ab65:0:0), session_name()\nI0000 00:00:1726895719.034403     825 tpu_compile_op_common.cc:245] Compilation of 8a1379109dc9ab65:0:0 with session name  took 5.465699641s and succeeded\nI0000 00:00:1726895719.045081     825 tpu_compilation_cache_interface.cc:475] TPU host compilation cache: compilation complete for cache_key(8a1379109dc9ab65:0:0), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_iterator_2681357801882899908\", property.function_library_fingerprint = 12043093643355222584, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"16,224,224,3,;16,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1726895719.045115     825 tpu_compilation_cache_interface.cc:541] After adding entry for key 8a1379109dc9ab65:0:0 with session_name  cache is 2 entries (147293944 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: val_loss improved from inf to 4.69761, saving model to /kaggle/working/model_checkpoint.keras\n\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 291ms/step - loss: 5.1120 - sparse_categorical_accuracy: 0.1464 - val_loss: 4.6976 - val_sparse_categorical_accuracy: 0.0129 - learning_rate: 0.0010\nEpoch 2/50\n\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 3.4955 - sparse_categorical_accuracy: 0.2791\nEpoch 2: val_loss did not improve from 4.69761\n\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 157ms/step - loss: 3.4955 - sparse_categorical_accuracy: 0.2790 - val_loss: 6.7163 - val_sparse_categorical_accuracy: 0.0194 - learning_rate: 0.0010\nEpoch 3/50\n\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 3.1935 - sparse_categorical_accuracy: 0.3426\nEpoch 3: val_loss did not improve from 4.69761\n\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 156ms/step - loss: 3.1941 - sparse_categorical_accuracy: 0.3424 - val_loss: 9.8291 - val_sparse_categorical_accuracy: 0.0409 - learning_rate: 0.0010\nEpoch 4/50\n\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 3.2443 - sparse_categorical_accuracy: 0.3011\nEpoch 4: val_loss did not improve from 4.69761\n\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 153ms/step - loss: 3.2438 - sparse_categorical_accuracy: 0.3012 - val_loss: 5.2361 - val_sparse_categorical_accuracy: 0.0431 - learning_rate: 0.0010\nEpoch 5/50\n\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 3.0098 - sparse_categorical_accuracy: 0.3448\nEpoch 5: val_loss did not improve from 4.69761\n\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 156ms/step - loss: 3.0081 - sparse_categorical_accuracy: 0.3451 - val_loss: 14.4491 - val_sparse_categorical_accuracy: 0.0453 - learning_rate: 5.0000e-04\nEpoch 6/50\n\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 2.6477 - sparse_categorical_accuracy: 0.4174\nEpoch 6: val_loss did not improve from 4.69761\n\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 157ms/step - loss: 2.6470 - sparse_categorical_accuracy: 0.4175 - val_loss: 9.4996 - val_sparse_categorical_accuracy: 0.1379 - learning_rate: 5.0000e-04\nEpoch 6: early stopping\nRestoring model weights from the end of the best epoch: 1.\n","output_type":"stream"}]},{"cell_type":"code","source":"model = model1","metadata":{"execution":{"iopub.status.busy":"2024-09-21T05:16:45.802130Z","iopub.execute_input":"2024-09-21T05:16:45.802422Z","iopub.status.idle":"2024-09-21T05:16:45.806230Z","shell.execute_reply.started":"2024-09-21T05:16:45.802391Z","shell.execute_reply":"2024-09-21T05:16:45.805443Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Make predictions\ntest_images = test_dataset.map(lambda image, idnum: image)\nprobabilities = model.predict(test_images)\npredictions = np.argmax(probabilities, axis=-1)\n\n# Prepare the submission file\ntest_ids = [idnum.numpy().decode('utf-8') for image, idnum in test_dataset.unbatch()]\nsubmission_df = pd.DataFrame({'id': test_ids, 'label': predictions})\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T05:16:45.807147Z","iopub.execute_input":"2024-09-21T05:16:45.807387Z","iopub.status.idle":"2024-09-21T05:17:12.654347Z","shell.execute_reply.started":"2024-09-21T05:16:45.807360Z","shell.execute_reply":"2024-09-21T05:17:12.652996Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-09-21 05:16:48.751387: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node sequential_1/dense_1/Add/ReadVariableOp.\nI0000 00:00:1726895809.226830     748 tpu_compilation_cache_interface.cc:441] TPU host compilation cache miss: cache_key(ba24e71c67e5b630:0:0), session_name()\n","output_type":"stream"},{"name":"stdout","text":"      2/Unknown \u001b[1m9s\u001b[0m 153ms/step","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1726895814.377299     748 tpu_compile_op_common.cc:245] Compilation of ba24e71c67e5b630:0:0 with session name  took 5.150426386s and succeeded\nI0000 00:00:1726895814.390271     748 tpu_compilation_cache_interface.cc:475] TPU host compilation cache: compilation complete for cache_key(ba24e71c67e5b630:0:0), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_distributed_14252349826022051486\", property.function_library_fingerprint = 14519655286161390278, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1726895814.390313     748 tpu_compilation_cache_interface.cc:541] After adding entry for key ba24e71c67e5b630:0:0 with session_name  cache is 3 entries (167708414 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"     57/Unknown \u001b[1m16s\u001b[0m 140ms/step","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1726895825.170925     816 tpu_compilation_cache_interface.cc:441] TPU host compilation cache miss: cache_key(519dc50b5b24210c:0:0), session_name()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 287ms/step\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1726895830.722307     816 tpu_compile_op_common.cc:245] Compilation of 519dc50b5b24210c:0:0 with session name  took 5.551343089s and succeeded\nI0000 00:00:1726895830.733582     816 tpu_compilation_cache_interface.cc:475] TPU host compilation cache: compilation complete for cache_key(519dc50b5b24210c:0:0), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_distributed_7532055726246147850\", property.function_library_fingerprint = 12630450583420436465, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"11,224,224,3,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1726895830.733607     816 tpu_compilation_cache_interface.cc:541] After adding entry for key 519dc50b5b24210c:0:0 with session_name  cache is 4 entries (188469553 bytes),  marked for eviction 0 entries (0 bytes).\n/usr/local/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"}]}]}